{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "executionInfo": {
          "elapsed": 64325,
          "status": "ok",
          "timestamp": 1762603174172,
          "user": {
            "displayName": "Hanif Nur Ilham Sanjaya",
            "userId": "10100194631026074709"
          },
          "user_tz": -420
        },
        "id": "G3j9Xp4LhN-O",
        "outputId": "3fee577a-4efe-4460-f161-98bb81fa960d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Initializing 1D-CNN Feature Extractor (Step 2) ---\n",
            "TensorFlow Version: 2.19.0\n",
            "Loading data from /content/drive/MyDrive/1 Skripsi/cnn_payload_data.npy...\n",
            "Loaded data shape: (9720, 10, 784)\n",
            "Loaded labels shape: (9720, 4)\n",
            "Reshaped X to: (9720, 7840, 1)\n",
            "Target label: 'application' with 6 classes.\n",
            "y shape after one-hot encoding: (9720, 6)\n",
            "Split map not found. Creating /content/drive/MyDrive/1 Skripsi/skrip16feb/train_test_split_map.csv...\n",
            "Created split map with 7776 TRAIN and 1944 TEST files.\n",
            "Loading split map from /content/drive/MyDrive/1 Skripsi/skrip16feb/train_test_split_map.csv...\n",
            "Original samples: 9720 -> Training samples allowed: 7776\n",
            "Training data: (6220, 7840, 1), Validation data: (1556, 7840, 1)\n",
            "Building 1D-CNN model...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7840</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7840</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1960</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1960</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,304</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">490</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">490</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">122</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15616</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,998,976</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ classifier_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">774</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7840\u001b[0m, \u001b[38;5;34m1\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7840\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_3 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1960\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1960\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m10,304\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_4 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m490\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m490\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m24,704\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_5 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m122\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15616\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ encoder_output (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m1,998,976\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ classifier_output (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │           \u001b[38;5;34m774\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,035,014</span> (7.76 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,035,014\u001b[0m (7.76 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,035,014</span> (7.76 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,035,014\u001b[0m (7.76 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "\n",
            "--- Starting 1D-CNN Training ---\n",
            "Epoch 1/100\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 98ms/step - accuracy: 0.5416 - loss: 1.2904 - val_accuracy: 0.6665 - val_loss: 0.8607\n",
            "Epoch 2/100\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - accuracy: 0.6593 - loss: 0.8150 - val_accuracy: 0.6999 - val_loss: 0.7298\n",
            "Epoch 3/100\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.6954 - loss: 0.6995 - val_accuracy: 0.7108 - val_loss: 0.6737\n",
            "Epoch 4/100\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.7274 - loss: 0.6128 - val_accuracy: 0.7494 - val_loss: 0.6249\n",
            "Epoch 5/100\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.7545 - loss: 0.5711 - val_accuracy: 0.7423 - val_loss: 0.6005\n",
            "Epoch 6/100\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - accuracy: 0.7743 - loss: 0.5154 - val_accuracy: 0.7526 - val_loss: 0.5753\n",
            "Epoch 7/100\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.7802 - loss: 0.5032 - val_accuracy: 0.7500 - val_loss: 0.5746\n",
            "Epoch 8/100\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.7758 - loss: 0.4783 - val_accuracy: 0.7526 - val_loss: 0.5618\n",
            "Epoch 9/100\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.7759 - loss: 0.4784 - val_accuracy: 0.7577 - val_loss: 0.5881\n",
            "Epoch 10/100\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - accuracy: 0.7915 - loss: 0.4573 - val_accuracy: 0.7596 - val_loss: 0.5953\n",
            "Epoch 11/100\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 63ms/step - accuracy: 0.7875 - loss: 0.4520 - val_accuracy: 0.7545 - val_loss: 0.5933\n",
            "Epoch 12/100\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.7866 - loss: 0.4515 - val_accuracy: 0.7564 - val_loss: 0.5870\n",
            "Epoch 13/100\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.8013 - loss: 0.4361 - val_accuracy: 0.7616 - val_loss: 0.5842\n",
            "Epoch 14/100\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.7891 - loss: 0.4400 - val_accuracy: 0.7551 - val_loss: 0.5760\n",
            "Epoch 15/100\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - accuracy: 0.7913 - loss: 0.4353 - val_accuracy: 0.7558 - val_loss: 0.5975\n",
            "Epoch 16/100\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.7912 - loss: 0.4355 - val_accuracy: 0.7596 - val_loss: 0.5696\n",
            "Epoch 17/100\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - accuracy: 0.7943 - loss: 0.4357 - val_accuracy: 0.7596 - val_loss: 0.5669\n",
            "Epoch 18/100\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 62ms/step - accuracy: 0.7938 - loss: 0.4252 - val_accuracy: 0.7571 - val_loss: 0.5838\n",
            "--- Training Complete ---\n",
            "Extracting and saving the encoder model...\n",
            "Encoder model saved to: /content/drive/MyDrive/1 Skripsi/skrip16feb/cnn_encoder_v3.keras\n",
            "Generating 128-dimension alpha'' features for all 9720 samples...\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
            "Generated features with shape: (9720, 128)\n",
            "New alpha'' (v3) component saved to: /content/drive/MyDrive/1 Skripsi/skrip16feb/alpha_double_prime_component_v3.csv\n",
            "\n",
            "--- 1D-CNN Feature Extractor Finished ---\n"
          ]
        }
      ],
      "source": [
        "# --- 1D-CNN Feature Extractor Script (Step 2) ---\n",
        "#\n",
        "# This script loads the raw payload data created by Step 1,\n",
        "# trains a 1D-CNN to classify applications (as suggested by\n",
        "# the research papers), and then saves the trained \"encoder\"\n",
        "# part of the model.\n",
        "#\n",
        "# It then uses this encoder to generate our new 128-dimension\n",
        "# alpha'' (alpha-double-prime) feature vector.\n",
        "#\n",
        "# This script requires TensorFlow/Keras.\n",
        "# In Colab, run: !pip install tensorflow\n",
        "\n",
        "print(\"--- Initializing 1D-CNN Feature Extractor (Step 2) ---\")\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")\n",
        "\n",
        "# --- PART 1: Configuration ---\n",
        "\n",
        "# --- File Paths ---\n",
        "BASE_PATH = \"/content/drive/MyDrive/1 Skripsi/\"\n",
        "DATA_FILE = os.path.join(BASE_PATH, \"cnn_payload_data.npy\")\n",
        "LABELS_FILE = os.path.join(BASE_PATH, \"cnn_payload_labels.csv\")\n",
        "\n",
        "# --- Output Files ---\n",
        "# The new feature set\n",
        "OUTPUT_ALPHA_V3_FILE = \"/content/drive/MyDrive/1 Skripsi/skrip16feb/alpha_double_prime_component_v3.csv\"\n",
        "# The saved encoder model for future use\n",
        "OUTPUT_ENCODER_MODEL_FILE = \"/content/drive/MyDrive/1 Skripsi/skrip16feb/cnn_encoder_v3.keras\"\n",
        "\n",
        "# --- Model Parameters ---\n",
        "# From Step 1, we know these are (10, 784)\n",
        "N_PACKETS = 10\n",
        "PAYLOAD_LEN = 784\n",
        "# We will reshape to (10 * 784, 1)\n",
        "INPUT_SHAPE = (N_PACKETS * PAYLOAD_LEN, 1) # (7840, 1)\n",
        "\n",
        "FEATURE_VECTOR_SIZE = 128 # The width of our new alpha'' feature\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# --- PART 2: Load & Prepare Data ---\n",
        "def load_and_prepare_data():\n",
        "    print(f\"Loading data from {DATA_FILE}...\")\n",
        "    X = np.load(DATA_FILE)\n",
        "    df_y = pd.read_csv(LABELS_FILE)\n",
        "\n",
        "    print(f\"Loaded data shape: {X.shape}\")\n",
        "    print(f\"Loaded labels shape: {df_y.shape}\")\n",
        "\n",
        "    # --- 1. Reshape X ---\n",
        "    # Reshape (samples, 10, 784) -> (samples, 7840, 1)\n",
        "    # This treats the 10 packets as one long 1D sequence\n",
        "    X_reshaped = X.reshape(X.shape[0], N_PACKETS * PAYLOAD_LEN, 1)\n",
        "    print(f\"Reshaped X to: {X_reshaped.shape}\")\n",
        "\n",
        "    # --- 2. Encode y ---\n",
        "    # We will train the CNN to predict the 'application'\n",
        "    y_labels = df_y['application']\n",
        "    num_classes = len(y_labels.unique())\n",
        "    print(f\"Target label: 'application' with {num_classes} classes.\")\n",
        "\n",
        "    # a. String labels to integer\n",
        "    le = LabelEncoder()\n",
        "    y_int = le.fit_transform(y_labels)\n",
        "\n",
        "    # b. Integer labels to one-hot vectors (for categorical_crossentropy)\n",
        "    y_categorical = to_categorical(y_int)\n",
        "\n",
        "    print(f\"y shape after one-hot encoding: {y_categorical.shape}\")\n",
        "\n",
        "    return X_reshaped, y_categorical, df_y, num_classes\n",
        "\n",
        "# --- PART 3: Build 1D-CNN Model ---\n",
        "def build_model(num_classes):\n",
        "    print(\"Building 1D-CNN model...\")\n",
        "\n",
        "    input_layer = Input(shape=INPUT_SHAPE)\n",
        "\n",
        "    # Convolutional Block 1\n",
        "    x = Conv1D(filters=32, kernel_size=7, activation='relu', padding='same')(input_layer)\n",
        "    x = MaxPooling1D(pool_size=4)(x)\n",
        "\n",
        "    # Convolutional Block 2\n",
        "    x = Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')(x)\n",
        "    x = MaxPooling1D(pool_size=4)(x)\n",
        "\n",
        "    # Convolutional Block 3\n",
        "    x = Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(x)\n",
        "    x = MaxPooling1D(pool_size=4)(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    # --- This is our Feature Vector ---\n",
        "    # We give it a name so we can easily extract it later\n",
        "    x = Dense(FEATURE_VECTOR_SIZE, activation='relu', name=\"encoder_output\")(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    # ----------------------------------\n",
        "\n",
        "    # Output classifier layer\n",
        "    output_layer = Dense(num_classes, activation='softmax', name=\"classifier_output\")(x)\n",
        "\n",
        "    # Create the full model\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    print(model.summary())\n",
        "    return model\n",
        "\n",
        "# --- PART 4: Main Execution ---\n",
        "def main():\n",
        "    if not all([os.path.exists(DATA_FILE), os.path.exists(LABELS_FILE)]):\n",
        "        print(f\"FATAL ERROR: Missing {DATA_FILE} or {LABELS_FILE}\")\n",
        "        print(\"Please run Step 1 (extract_cnn_payloads.py) first.\")\n",
        "        return\n",
        "\n",
        "    # --- 1. Load Data ---\n",
        "    X_full, y_full, df_labels, num_classes = load_and_prepare_data()\n",
        "\n",
        "    # --- 2. Split Data for Training ---\n",
        "    # --- 2. Split Data for Training (FIXED) ---\n",
        "    SPLIT_MAP_FILE = \"/content/drive/MyDrive/1 Skripsi/skrip16feb/train_test_split_map.csv\"\n",
        "    \n",
        "    # Check if map exists, if not create it\n",
        "    if not os.path.exists(SPLIT_MAP_FILE):\n",
        "        print(f\"Split map not found. Creating {SPLIT_MAP_FILE}...\")\n",
        "        unique_files = df_labels['filename'].unique()\n",
        "        train_files, test_files = train_test_split(unique_files, test_size=0.2, random_state=42)\n",
        "        df_train = pd.DataFrame({'filename': train_files, 'split_group': 'TRAIN'})\n",
        "        df_test = pd.DataFrame({'filename': test_files, 'split_group': 'TEST'})\n",
        "        pd.concat([df_train, df_test], ignore_index=True).to_csv(SPLIT_MAP_FILE, index=False)\n",
        "        print(f\"Created split map with {len(train_files)} TRAIN and {len(test_files)} TEST files.\")\n",
        "    \n",
        "    print(f\"Loading split map from {SPLIT_MAP_FILE}...\")\n",
        "    df_split = pd.read_csv(SPLIT_MAP_FILE)\n",
        "\n",
        "    # Merge labels with split map to get 'split_group' for each sample\n",
        "    df_merged = pd.merge(df_labels, df_split, on='filename', how='left')\n",
        "\n",
        "    # Filter: Keep ONLY 'TRAIN' files for CNN training\n",
        "    train_mask = (df_merged['split_group'] == 'TRAIN').values\n",
        "    X_train_full = X_full[train_mask]\n",
        "    y_train_full = y_full[train_mask]\n",
        "    \n",
        "    print(f\"Original samples: {len(X_full)} -> Training samples allowed: {len(X_train_full)}\")\n",
        "\n",
        "    # Now split the 'TRAIN' group into Train and Validation for the CNN\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train_full, y_train_full,\n",
        "        test_size=0.2, # 20% of the TRAIN group for internal validation\n",
        "        random_state=RANDOM_STATE,\n",
        "        stratify=y_train_full\n",
        "    )\n",
        "\n",
        "\n",
        "    print(f\"Training data: {X_train.shape}, Validation data: {X_val.shape}\")\n",
        "\n",
        "    # --- 3. Build & Train Model ---\n",
        "    model = build_model(num_classes)\n",
        "\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10, # Stop if val_loss doesn't improve for 10 epochs\n",
        "        restore_best_weights=True # Restore the best model\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Starting 1D-CNN Training ---\")\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=100, # Max epochs\n",
        "        batch_size=64,\n",
        "        validation_data=(X_val, y_val),\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "    print(\"--- Training Complete ---\")\n",
        "\n",
        "    # --- 4. Create and Save the Encoder ---\n",
        "    print(\"Extracting and saving the encoder model...\")\n",
        "\n",
        "    # Create a new model that ends at our named \"encoder_output\" layer\n",
        "    encoder_model = Model(\n",
        "        inputs=model.input,\n",
        "        outputs=model.get_layer(\"encoder_output\").output\n",
        "    )\n",
        "\n",
        "    encoder_model.save(OUTPUT_ENCODER_MODEL_FILE)\n",
        "    print(f\"Encoder model saved to: {OUTPUT_ENCODER_MODEL_FILE}\")\n",
        "\n",
        "    # --- 5. Generate and Save alpha'' Features ---\n",
        "    print(f\"Generating {FEATURE_VECTOR_SIZE}-dimension alpha'' features for all {X_full.shape[0]} samples...\")\n",
        "\n",
        "    # Use the encoder to predict (extract features) on the *entire* dataset\n",
        "    alpha_prime_prime_features = encoder_model.predict(X_full, batch_size=128)\n",
        "\n",
        "    print(f\"Generated features with shape: {alpha_prime_prime_features.shape}\")\n",
        "\n",
        "    # Create a DataFrame for the new features\n",
        "    alpha_cols = [f'alpha_pp_{i}' for i in range(FEATURE_VECTOR_SIZE)]\n",
        "    df_alpha_pp = pd.DataFrame(alpha_prime_prime_features, columns=alpha_cols)\n",
        "\n",
        "    # Combine with the original labels (for merging later)\n",
        "    # We take the 'filename' from df_labels\n",
        "    df_final_alpha = pd.concat([df_labels['filename'], df_alpha_pp], axis=1)\n",
        "\n",
        "    # Save to CSV\n",
        "    df_final_alpha.to_csv(OUTPUT_ALPHA_V3_FILE, index=False)\n",
        "    print(f\"New alpha'' (v3) component saved to: {OUTPUT_ALPHA_V3_FILE}\")\n",
        "    print(\"\\n--- 1D-CNN Feature Extractor Finished ---\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if not os.path.exists(\"/content/drive/MyDrive\"):\n",
        "        print(\"Please mount your Google Drive first!\")\n",
        "    else:\n",
        "        main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIaDWEFDhewr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMtGzmyBSkqfQHnBa9TFkwI",
      "gpuType": "T4",
      "mount_file_id": "1c9dyq8Rm1A60ALwpkt8gxZxjiEQYbeZb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
