{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2zqeTrosMs3",
        "outputId": "104bba0d-e428-4d51-da9f-2b7643d76ca8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Initializing HFV Hierarchical Classification ---\n",
            "\n",
            "--- Loading Full Dataset from /content/drive/MyDrive/1 Skripsi/skrip16feb/HFV_dataset.csv ---\n",
            "\n",
            "--- Loading Split Map: /content/drive/MyDrive/1 Skripsi/skrip16feb/alpha_train_test_split_map.csv ---\n",
            "Data shape after merge: (12555, 254)\n",
            "\n",
            "########################################\n",
            " STEP 1: BINARY CLASSIFICATION (VPN vs NonVPN) \n",
            "########################################\n",
            "Binary Train Samples: 10042, Test Samples: 2513\n",
            " > Training Random Forest on Alpha'' (Î±'') only (128 feats)...\n",
            " > Training Random Forest on Delta (Î´) only (121 feats)...\n",
            " > Training Random Forest on Alpha'' + Delta (249 feats)...\n",
            " > Training Random Forest on Alpha'' + Gamma' (128 feats)...\n",
            " > Training Random Forest on Delta + Gamma' (121 feats)...\n",
            " > Training Random Forest on Full (Î±'' + Î´ + Î³') (249 feats)...\n",
            " > Training XGBoost on Alpha'' (Î±'') only (128 feats)...\n",
            " > Training XGBoost on Delta (Î´) only (121 feats)...\n",
            " > Training XGBoost on Alpha'' + Delta (249 feats)...\n",
            " > Training XGBoost on Alpha'' + Gamma' (128 feats)...\n",
            " > Training XGBoost on Delta + Gamma' (121 feats)...\n",
            " > Training XGBoost on Full (Î±'' + Î´ + Î³') (249 feats)...\n",
            "\n",
            "====================================================================================================\n",
            "--- FINAL RESULTS: Binary Type ---\n",
            "====================================================================================================\n",
            "\n",
            "Model           | Feature Set            | Acc    | F1(W)  | F1(Mac)  | Time \n",
            "--------------------------------------------------------------------------------\n",
            "XGBoost         | Delta (Î´) only         | 0.9725 | 0.9726 | 0.9606   | 7.7  \n",
            "XGBoost         | Delta + Gamma'         | 0.9725 | 0.9726 | 0.9606   | 8.2  \n",
            "XGBoost         | Alpha'' + Delta        | 0.9706 | 0.9706 | 0.9578   | 16.7 \n",
            "XGBoost         | Full (Î±'' + Î´ + Î³')    | 0.9706 | 0.9706 | 0.9578   | 14.7 \n",
            "Random Forest   | Delta (Î´) only         | 0.9522 | 0.9533 | 0.9346   | 15.3 \n",
            "Random Forest   | Delta + Gamma'         | 0.9522 | 0.9533 | 0.9346   | 14.6 \n",
            "Random Forest   | Alpha'' + Delta        | 0.9308 | 0.9331 | 0.9077   | 9.6  \n",
            "Random Forest   | Full (Î±'' + Î´ + Î³')    | 0.9308 | 0.9331 | 0.9077   | 9.7  \n",
            "XGBoost         | Alpha'' (Î±'') only     | 0.9184 | 0.9188 | 0.8839   | 6.7  \n",
            "XGBoost         | Alpha'' + Gamma'       | 0.9184 | 0.9188 | 0.8839   | 7.0  \n",
            "Random Forest   | Alpha'' (Î±'') only     | 0.8902 | 0.8958 | 0.8602   | 7.6  \n",
            "Random Forest   | Alpha'' + Gamma'       | 0.8902 | 0.8958 | 0.8602   | 6.8  \n",
            "\n",
            "--- ðŸ† Top 3 Models for Binary Type ---\n",
            "1. XGBoost [Delta (Î´) only] - Acc: 0.9725, Macro F1: 0.9606\n",
            "   Params: {'subsample': 0.8, 'n_estimators': 200, 'max_depth': 6, 'learning_rate': 0.2}\n",
            "2. XGBoost [Delta + Gamma'] - Acc: 0.9725, Macro F1: 0.9606\n",
            "   Params: {'subsample': 0.8, 'n_estimators': 200, 'max_depth': 6, 'learning_rate': 0.2}\n",
            "3. XGBoost [Alpha'' + Delta] - Acc: 0.9706, Macro F1: 0.9578\n",
            "   Params: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 10, 'learning_rate': 0.2}\n",
            "\n",
            "########################################\n",
            " STEP 2: VPN-SPECIFIC CLASSIFICATION \n",
            "########################################\n",
            "Filtering for specific apps: ['Skype', 'BitTorrent', 'Hangout', 'Facebook', 'YouTube', 'Email']\n",
            "VPN Filtered Data - Total: 2411\n",
            "VPN Train Samples: 1919, Test Samples: 492\n",
            "\n",
            "--- Target: category (Within VPN) ---\n",
            " > Training Random Forest on Alpha'' (Î±'') only (128 feats)...\n",
            " > Training Random Forest on Delta (Î´) only (121 feats)...\n",
            " > Training Random Forest on Alpha'' + Delta (249 feats)...\n",
            " > Training Random Forest on Alpha'' + Gamma' (128 feats)...\n",
            " > Training Random Forest on Delta + Gamma' (121 feats)...\n",
            " > Training Random Forest on Full (Î±'' + Î´ + Î³') (249 feats)...\n",
            " > Training XGBoost on Alpha'' (Î±'') only (128 feats)...\n",
            " > Training XGBoost on Delta (Î´) only (121 feats)...\n",
            " > Training XGBoost on Alpha'' + Delta (249 feats)...\n",
            " > Training XGBoost on Alpha'' + Gamma' (128 feats)...\n",
            " > Training XGBoost on Delta + Gamma' (121 feats)...\n",
            " > Training XGBoost on Full (Î±'' + Î´ + Î³') (249 feats)...\n",
            "\n",
            "====================================================================================================\n",
            "--- FINAL RESULTS: VPN category ---\n",
            "====================================================================================================\n",
            "\n",
            "Model           | Feature Set            | Acc    | F1(W)  | F1(Mac)  | Time \n",
            "--------------------------------------------------------------------------------\n",
            "XGBoost         | Alpha'' + Delta        | 0.9329 | 0.9331 | 0.9111   | 31.0 \n",
            "XGBoost         | Full (Î±'' + Î´ + Î³')    | 0.9329 | 0.9331 | 0.9111   | 32.0 \n",
            "Random Forest   | Alpha'' + Delta        | 0.9248 | 0.9261 | 0.9107   | 3.2  \n",
            "Random Forest   | Full (Î±'' + Î´ + Î³')    | 0.9248 | 0.9261 | 0.9107   | 3.3  \n",
            "XGBoost         | Delta (Î´) only         | 0.9289 | 0.9300 | 0.9013   | 17.2 \n",
            "XGBoost         | Delta + Gamma'         | 0.9289 | 0.9300 | 0.9013   | 17.2 \n",
            "Random Forest   | Delta (Î´) only         | 0.9187 | 0.9202 | 0.8995   | 4.2  \n",
            "Random Forest   | Delta + Gamma'         | 0.9187 | 0.9202 | 0.8995   | 4.2  \n",
            "Random Forest   | Alpha'' (Î±'') only     | 0.8862 | 0.8836 | 0.8544   | 2.0  \n",
            "Random Forest   | Alpha'' + Gamma'       | 0.8862 | 0.8836 | 0.8544   | 2.0  \n",
            "XGBoost         | Alpha'' (Î±'') only     | 0.8841 | 0.8816 | 0.8499   | 14.9 \n",
            "XGBoost         | Alpha'' + Gamma'       | 0.8841 | 0.8816 | 0.8499   | 16.2 \n",
            "\n",
            "--- ðŸ† Top 3 Models for VPN category ---\n",
            "1. XGBoost [Alpha'' + Delta] - Acc: 0.9329, Macro F1: 0.9111\n",
            "   Params: {'subsample': 0.8, 'n_estimators': 200, 'max_depth': 10, 'learning_rate': 0.1}\n",
            "2. XGBoost [Full (Î±'' + Î´ + Î³')] - Acc: 0.9329, Macro F1: 0.9111\n",
            "   Params: {'subsample': 0.8, 'n_estimators': 200, 'max_depth': 10, 'learning_rate': 0.1}\n",
            "3. Random Forest [Alpha'' + Delta] - Acc: 0.9248, Macro F1: 0.9107\n",
            "   Params: {'n_estimators': 200, 'min_samples_split': 10, 'max_depth': 30, 'criterion': 'gini'}\n",
            "\n",
            "--- Target: application (Within VPN) ---\n",
            " > Training Random Forest on Alpha'' (Î±'') only (128 feats)...\n",
            " > Training Random Forest on Delta (Î´) only (121 feats)...\n",
            " > Training Random Forest on Alpha'' + Delta (249 feats)...\n",
            " > Training Random Forest on Alpha'' + Gamma' (128 feats)...\n",
            " > Training Random Forest on Delta + Gamma' (121 feats)...\n",
            " > Training Random Forest on Full (Î±'' + Î´ + Î³') (249 feats)...\n",
            " > Training XGBoost on Alpha'' (Î±'') only (128 feats)...\n",
            " > Training XGBoost on Delta (Î´) only (121 feats)...\n",
            " > Training XGBoost on Alpha'' + Delta (249 feats)...\n",
            " > Training XGBoost on Alpha'' + Gamma' (128 feats)...\n",
            " > Training XGBoost on Delta + Gamma' (121 feats)...\n",
            " > Training XGBoost on Full (Î±'' + Î´ + Î³') (249 feats)...\n",
            "\n",
            "====================================================================================================\n",
            "--- FINAL RESULTS: VPN application ---\n",
            "====================================================================================================\n",
            "\n",
            "Model           | Feature Set            | Acc    | F1(W)  | F1(Mac)  | Time \n",
            "--------------------------------------------------------------------------------\n",
            "Random Forest   | Alpha'' + Delta        | 0.9533 | 0.9530 | 0.9421   | 3.0  \n",
            "Random Forest   | Full (Î±'' + Î´ + Î³')    | 0.9533 | 0.9530 | 0.9421   | 2.9  \n",
            "Random Forest   | Delta (Î´) only         | 0.9309 | 0.9303 | 0.9106   | 2.7  \n",
            "Random Forest   | Delta + Gamma'         | 0.9309 | 0.9303 | 0.9106   | 2.6  \n",
            "XGBoost         | Alpha'' + Delta        | 0.9329 | 0.9329 | 0.9094   | 19.5 \n",
            "XGBoost         | Full (Î±'' + Î´ + Î³')    | 0.9329 | 0.9329 | 0.9094   | 18.9 \n",
            "XGBoost         | Delta (Î´) only         | 0.9309 | 0.9301 | 0.9046   | 16.6 \n",
            "XGBoost         | Delta + Gamma'         | 0.9309 | 0.9301 | 0.9046   | 16.2 \n",
            "Random Forest   | Alpha'' (Î±'') only     | 0.9106 | 0.9095 | 0.8790   | 1.4  \n",
            "Random Forest   | Alpha'' + Gamma'       | 0.9106 | 0.9095 | 0.8790   | 1.4  \n",
            "XGBoost         | Alpha'' (Î±'') only     | 0.9085 | 0.9076 | 0.8714   | 10.0 \n",
            "XGBoost         | Alpha'' + Gamma'       | 0.9085 | 0.9076 | 0.8714   | 9.9  \n",
            "\n",
            "--- ðŸ† Top 3 Models for VPN application ---\n",
            "1. Random Forest [Alpha'' + Delta] - Acc: 0.9533, Macro F1: 0.9421\n",
            "   Params: {'n_estimators': 200, 'min_samples_split': 5, 'max_depth': None, 'criterion': 'gini'}\n",
            "2. Random Forest [Full (Î±'' + Î´ + Î³')] - Acc: 0.9533, Macro F1: 0.9421\n",
            "   Params: {'n_estimators': 200, 'min_samples_split': 5, 'max_depth': None, 'criterion': 'gini'}\n",
            "3. Random Forest [Delta (Î´) only] - Acc: 0.9309, Macro F1: 0.9106\n",
            "   Params: {'n_estimators': 100, 'min_samples_split': 2, 'max_depth': 20, 'criterion': 'entropy'}\n"
          ]
        }
      ],
      "source": [
        "# --- PESV v3 \"Championship\" with Hyperparameter Tuning (Hierarchical Flow) ---\n",
        "#\n",
        "# Workflow:\n",
        "# 1. Binary Classification (VPN vs Non-VPN) on FULL dataset.\n",
        "# 2. Filter for VPN traffic only.\n",
        "# 3. Filter for specific target applications.\n",
        "# 4. Classify Category and Application within the VPN subset.\n",
        "\n",
        "print(\"--- Initializing HFV Hierarchical Classification ---\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import warnings\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Try importing XGBoost\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    HAS_XGB = True\n",
        "except ImportError:\n",
        "    print(\"WARNING: XGBoost not installed. Skipping XGBoost.\")\n",
        "    HAS_XGB = False\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# --- PART 1: Configuration ---\n",
        "\n",
        "BASE_PATH = \"/content/drive/MyDrive/1 Skripsi/skrip16feb\"\n",
        "FINAL_PESV_FILE = os.path.join(BASE_PATH, \"HFV_dataset.csv\")\n",
        "SPLIT_MAP_FILE = os.path.join(BASE_PATH, \"alpha_train_test_split_map.csv\")\n",
        "\n",
        "# Target Applications for the VPN Step\n",
        "# UPDATED: Removed 'VPN_' prefix to match dataset labels found in 'application' column.\n",
        "TARGET_VPN_APPS = [\n",
        "    'Skype', 'BitTorrent', 'Hangout',\n",
        "    'Facebook', 'YouTube', 'Email'\n",
        "]\n",
        "\n",
        "TEST_SET_SIZE = 0.2\n",
        "RANDOM_STATE = 42\n",
        "CV_FOLDS = 3\n",
        "N_ITER_SEARCH = 10\n",
        "\n",
        "# --- PART 2: Model & Parameter Definitions ---\n",
        "\n",
        "MODEL_CONFIGS = {\n",
        "    \"Random Forest\": {\n",
        "        \"model\": RandomForestClassifier(random_state=RANDOM_STATE, class_weight=\"balanced\"),\n",
        "        \"params\": {\n",
        "            \"classifier__n_estimators\": [100, 200, 300],\n",
        "            \"classifier__max_depth\": [None, 10, 20, 30],\n",
        "            \"classifier__min_samples_split\": [2, 5, 10],\n",
        "            \"classifier__criterion\": [\"gini\", \"entropy\"]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "if HAS_XGB:\n",
        "    MODEL_CONFIGS[\"XGBoost\"] = {\n",
        "        \"model\": XGBClassifier(random_state=RANDOM_STATE, eval_metric='mlogloss'),\n",
        "        \"params\": {\n",
        "            \"classifier__n_estimators\": [100, 200, 300],\n",
        "            \"classifier__learning_rate\": [0.01, 0.1, 0.2],\n",
        "            \"classifier__max_depth\": [3, 6, 10],\n",
        "            \"classifier__subsample\": [0.8, 1.0]\n",
        "        }\n",
        "    }\n",
        "\n",
        "# --- PART 3: Data Loading Helper ---\n",
        "\n",
        "def load_data_and_features():\n",
        "    print(f\"\\n--- Loading Full Dataset from {FINAL_PESV_FILE} ---\")\n",
        "    if not os.path.exists(FINAL_PESV_FILE):\n",
        "        print(f\"FATAL ERROR: Could not find dataset at '{FINAL_PESV_FILE}'\")\n",
        "        return None, None\n",
        "\n",
        "    df = pd.read_csv(FINAL_PESV_FILE)\n",
        "\n",
        "    # Define Feature Columns\n",
        "    all_cols = set(df.columns)\n",
        "    alpha_cols = sorted([c for c in all_cols if c.startswith('alpha_pp_')])\n",
        "    delta_cols = sorted([c for c in all_cols if c.startswith(('c2s_', 's2c_', 'flow_', 'total_'))])\n",
        "    gamma_cols = sorted([c for c in all_cols if c.startswith('burst_')])\n",
        "\n",
        "    feature_sets = {\n",
        "        \"Alpha'' (Î±'') only\": alpha_cols,\n",
        "        \"Delta (Î´) only\": delta_cols,\n",
        "        \"Gamma' (Î³') only\": gamma_cols,\n",
        "        \"Alpha'' + Delta\": alpha_cols + delta_cols,\n",
        "        \"Alpha'' + Gamma'\": alpha_cols + gamma_cols,\n",
        "        \"Delta + Gamma'\": delta_cols + gamma_cols,\n",
        "        \"Full (Î±'' + Î´ + Î³')\": alpha_cols + delta_cols + gamma_cols,\n",
        "    }\n",
        "\n",
        "    return df, feature_sets\n",
        "\n",
        "# --- PART 4: Tuned Classification Task ---\n",
        "\n",
        "def run_tuned_classification(df_train, df_test, target_label, feature_set_name, feature_cols, model_name, config):\n",
        "    print(f\" > Training {model_name} on {feature_set_name} ({len(feature_cols)} feats)...\")\n",
        "\n",
        "    X_train = df_train[feature_cols].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "    y_train = df_train[target_label]\n",
        "    X_test = df_test[feature_cols].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "    y_test = df_test[target_label]\n",
        "\n",
        "    # Encode labels (Union of train/test to handle missing classes in splits)\n",
        "    le = LabelEncoder()\n",
        "    all_labels = pd.concat([y_train, y_test], axis=0)\n",
        "    le.fit(all_labels)\n",
        "    y_train_encoded = le.transform(y_train)\n",
        "    y_test_encoded = le.transform(y_test)\n",
        "    class_names = [str(c) for c in le.classes_]\n",
        "\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('classifier', config[\"model\"])\n",
        "    ])\n",
        "\n",
        "    search = RandomizedSearchCV(\n",
        "        pipeline,\n",
        "        param_distributions=config[\"params\"],\n",
        "        n_iter=N_ITER_SEARCH,\n",
        "        scoring='f1_macro',\n",
        "        cv=CV_FOLDS,\n",
        "        n_jobs=-1,\n",
        "        random_state=RANDOM_STATE,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        search.fit(X_train, y_train_encoded)\n",
        "        best_model = search.best_estimator_\n",
        "        best_params = search.best_params_\n",
        "        y_pred = best_model.predict(X_test)\n",
        "\n",
        "        report = classification_report(y_test_encoded, y_pred, target_names=class_names, output_dict=True)\n",
        "\n",
        "        return {\n",
        "            'model': model_name,\n",
        "            'feature_set': feature_set_name,\n",
        "            'accuracy': report['accuracy'],\n",
        "            'f1_weighted': report['weighted avg']['f1-score'],\n",
        "            'f1_macro': report['macro avg']['f1-score'],\n",
        "            'time': time.time() - start_time,\n",
        "            'best_params': str(best_params).replace(\"classifier__\", \"\")\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"    ERROR in training: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- PART 5: Main Orchestration ---\n",
        "\n",
        "def print_results(task_name, metrics_list):\n",
        "    print(f\"\\n{'='*100}\")\n",
        "    print(f\"--- FINAL RESULTS: {task_name} ---\")\n",
        "    print(f\"{'='*100}\\n\")\n",
        "\n",
        "    sorted_res = sorted([m for m in metrics_list if m], key=lambda x: x['f1_macro'], reverse=True)\n",
        "\n",
        "    print(f\"{'Model':<15} | {'Feature Set':<22} | {'Acc':<6} | {'F1(W)':<6} | {'F1(Mac)':<8} | {'Time':<5}\")\n",
        "    print(\"-\" * 80)\n",
        "    for r in sorted_res:\n",
        "        print(f\"{r['model']:<15} | {r['feature_set']:<22} | {r['accuracy']:.4f} | {r['f1_weighted']:.4f} | {r['f1_macro']:.4f}   | {r['time']:<5.1f}\")\n",
        "\n",
        "    print(f\"\\n--- ðŸ† Top 3 Models for {task_name} ---\")\n",
        "    for i, r in enumerate(sorted_res[:3]):\n",
        "        print(f\"{i+1}. {r['model']} [{r['feature_set']}] - Acc: {r['accuracy']:.4f}, Macro F1: {r['f1_macro']:.4f}\")\n",
        "        print(f\"   Params: {r['best_params']}\")\n",
        "\n",
        "def main():\n",
        "    # 1. Load Data\n",
        "    df, feature_sets = load_data_and_features()\n",
        "    if df is None: return\n",
        "\n",
        "    # 2. Merge with Split Map (Global Split)\n",
        "    if os.path.exists(SPLIT_MAP_FILE):\n",
        "        print(f\"\\n--- Loading Split Map: {SPLIT_MAP_FILE} ---\")\n",
        "        split_map = pd.read_csv(SPLIT_MAP_FILE)\n",
        "        # Normalization (Fix for Case Sensitivity)\n",
        "        split_map.columns = [c.lower().strip() for c in split_map.columns]\n",
        "\n",
        "        col_map = {'filename': None, 'split': None}\n",
        "        for c in split_map.columns:\n",
        "            if c in ['filename', 'file', 'pcap']: col_map['filename'] = c\n",
        "            if c in ['split', 'set', 'partition', 'split_group']: col_map['split'] = c\n",
        "\n",
        "        if not col_map['filename'] or not col_map['split']:\n",
        "            print(\"ERROR: Invalid Split Map Columns\")\n",
        "            return\n",
        "\n",
        "        split_map = split_map.rename(columns={col_map['filename']: 'filename', col_map['split']: 'split'})\n",
        "        split_map['split'] = split_map['split'].str.lower() # Vital Fix\n",
        "\n",
        "        df_merged = pd.merge(df, split_map[['filename', 'split']], on='filename', how='inner')\n",
        "        print(f\"Data shape after merge: {df_merged.shape}\")\n",
        "    else:\n",
        "        print(\"FATAL ERROR: Split map not found. Aborting to prevent leakage.\")\n",
        "        return\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # STEP 1: BINARY CLASSIFICATION (VPN vs NonVPN)\n",
        "    # ---------------------------------------------------------\n",
        "    print(f\"\\n{'#'*40}\")\n",
        "    print(\" STEP 1: BINARY CLASSIFICATION (VPN vs NonVPN) \")\n",
        "    print(f\"{'#'*40}\")\n",
        "\n",
        "    df_train = df_merged[df_merged['split'] == 'train']\n",
        "    df_test = df_merged[df_merged['split'] == 'test']\n",
        "    print(f\"Binary Train Samples: {len(df_train)}, Test Samples: {len(df_test)}\")\n",
        "\n",
        "    binary_metrics = []\n",
        "    for model_name, config in MODEL_CONFIGS.items():\n",
        "        for fs_name, fs_cols in feature_sets.items():\n",
        "            if fs_cols:\n",
        "                m = run_tuned_classification(df_train, df_test, 'binary_type', fs_name, fs_cols, model_name, config)\n",
        "                if m: binary_metrics.append(m)\n",
        "\n",
        "    print_results(\"Binary Type\", binary_metrics)\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # STEP 2: VPN-SPECIFIC CLASSIFICATION (Category & App)\n",
        "    # ---------------------------------------------------------\n",
        "    print(f\"\\n{'#'*40}\")\n",
        "    print(\" STEP 2: VPN-SPECIFIC CLASSIFICATION \")\n",
        "    print(f\"{'#'*40}\")\n",
        "\n",
        "    # Filter: Keep only VPN rows\n",
        "    df_vpn = df_merged[df_merged['binary_type'] == 'VPN'].copy()\n",
        "\n",
        "    # Filter: Keep only Target Apps\n",
        "    print(f\"Filtering for specific apps: {TARGET_VPN_APPS}\")\n",
        "    df_vpn_filtered = df_vpn[df_vpn['application'].isin(TARGET_VPN_APPS)]\n",
        "\n",
        "    df_train_vpn = df_vpn_filtered[df_vpn_filtered['split'] == 'train']\n",
        "    df_test_vpn = df_vpn_filtered[df_vpn_filtered['split'] == 'test']\n",
        "\n",
        "    print(f\"VPN Filtered Data - Total: {len(df_vpn_filtered)}\")\n",
        "    print(f\"VPN Train Samples: {len(df_train_vpn)}, Test Samples: {len(df_test_vpn)}\")\n",
        "\n",
        "    if len(df_train_vpn) == 0 or len(df_test_vpn) == 0:\n",
        "        print(\"WARNING: No samples left after filtering for specific VPN apps. Check dataset labels.\")\n",
        "        return\n",
        "\n",
        "    # Run Tasks\n",
        "    for task in ['category', 'application']:\n",
        "        print(f\"\\n--- Target: {task} (Within VPN) ---\")\n",
        "        task_metrics = []\n",
        "        for model_name, config in MODEL_CONFIGS.items():\n",
        "            for fs_name, fs_cols in feature_sets.items():\n",
        "                if fs_cols:\n",
        "                    m = run_tuned_classification(df_train_vpn, df_test_vpn, task, fs_name, fs_cols, model_name, config)\n",
        "                    if m: task_metrics.append(m)\n",
        "\n",
        "        print_results(f\"VPN {task}\", task_metrics)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8_8be_MtGEH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V6E1",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
