{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing PESV v3 Tuned Championship ---\n",
      "\n",
      "--- Loading Data for Mode: VPN_ONLY ---\n",
      "\n",
      "--- Loading Train/Test Split Map from /content/drive/MyDrive/1 Skripsi/skrip16feb/VPNOnly-train_test_split_map.csv ---\n",
      "Data shape after merging with split map: (2623, 209)\n",
      "Train Set: 2098 samples\n",
      "Test Set:  525 samples\n",
      "\n",
      "================================================================================\n",
      "--- TUNING AND EVALUATING TARGET: category ---\n",
      "================================================================================\n",
      " > Training Random Forest on Alpha'' (\u03b1'') only (128 feats)...\n",
      " > Training Random Forest on Delta (\u03b4) only (40 feats)...\n",
      " > Training Random Forest on Gamma' (\u03b3') only (36 feats)...\n",
      " > Training Random Forest on Alpha'' + Delta (168 feats)...\n",
      " > Training Random Forest on Alpha'' + Gamma' (164 feats)...\n",
      " > Training Random Forest on Delta + Gamma' (76 feats)...\n",
      " > Training Random Forest on Full (\u03b1'' + \u03b4 + \u03b3') (204 feats)...\n",
      " > Training XGBoost on Alpha'' (\u03b1'') only (128 feats)...\n",
      " > Training XGBoost on Delta (\u03b4) only (40 feats)...\n",
      " > Training XGBoost on Gamma' (\u03b3') only (36 feats)...\n"
     ]
    }
   ],
   "source": [
    "# --- PESV v3 \"Championship\" with Hyperparameter Tuning (Hierarchical Flow) ---\n",
    "#\n",
    "# Workflow:\n",
    "# 1. Binary Classification (VPN vs Non-VPN) on FULL dataset.\n",
    "# 2. Filter for VPN traffic only.\n",
    "# 3. Filter for specific target applications.\n",
    "# 4. Classify Category and Application within the VPN subset.\n",
    "\n",
    "print(\"--- Initializing HFV Hierarchical Classification ---\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Try importing XGBoost\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    HAS_XGB = True\n",
    "except ImportError:\n",
    "    print(\"WARNING: XGBoost not installed. Skipping XGBoost.\")\n",
    "    HAS_XGB = False\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- PART 1: Configuration ---\n",
    "\n",
    "BASE_PATH = \"/content/drive/MyDrive/1 Skripsi/skrip16feb\"\n",
    "FINAL_PESV_FILE = os.path.join(BASE_PATH, \"HFV_dataset.csv\")\n",
    "SPLIT_MAP_FILE = os.path.join(BASE_PATH, \"alpha_train_test_split_map.csv\")\n",
    "\n",
    "# Target Applications for the VPN Step\n",
    "# UPDATED: Removed 'VPN_' prefix to match dataset labels found in 'application' column.\n",
    "TARGET_VPN_APPS = [\n",
    "    'Skype', 'BitTorrent', 'Hangout',\n",
    "    'Facebook', 'YouTube', 'Email'\n",
    "]\n",
    "\n",
    "TEST_SET_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "CV_FOLDS = 3\n",
    "N_ITER_SEARCH = 10\n",
    "\n",
    "# --- PART 2: Model & Parameter Definitions ---\n",
    "\n",
    "MODEL_CONFIGS = {\n",
    "    \"Random Forest\": {\n",
    "        \"model\": RandomForestClassifier(random_state=RANDOM_STATE, class_weight=\"balanced\"),\n",
    "        \"params\": {\n",
    "            \"classifier__n_estimators\": [100, 200, 300],\n",
    "            \"classifier__max_depth\": [None, 10, 20, 30],\n",
    "            \"classifier__min_samples_split\": [2, 5, 10],\n",
    "            \"classifier__criterion\": [\"gini\", \"entropy\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "if HAS_XGB:\n",
    "    MODEL_CONFIGS[\"XGBoost\"] = {\n",
    "        \"model\": XGBClassifier(random_state=RANDOM_STATE, eval_metric='mlogloss'),\n",
    "        \"params\": {\n",
    "            \"classifier__n_estimators\": [100, 200, 300],\n",
    "            \"classifier__learning_rate\": [0.01, 0.1, 0.2],\n",
    "            \"classifier__max_depth\": [3, 6, 10],\n",
    "            \"classifier__subsample\": [0.8, 1.0]\n",
    "        }\n",
    "    }\n",
    "\n",
    "# --- PART 3: Data Loading Helper ---\n",
    "\n",
    "def load_data_and_features():\n",
    "    print(f\"\\n--- Loading Full Dataset from {FINAL_PESV_FILE} ---\")\n",
    "    if not os.path.exists(FINAL_PESV_FILE):\n",
    "        print(f\"FATAL ERROR: Could not find dataset at '{FINAL_PESV_FILE}'\")\n",
    "        return None, None\n",
    "\n",
    "    df = pd.read_csv(FINAL_PESV_FILE)\n",
    "    \n",
    "    # Define Feature Columns\n",
    "    all_cols = set(df.columns)\n",
    "    alpha_cols = sorted([c for c in all_cols if c.startswith('alpha_pp_')])\n",
    "    delta_cols = sorted([c for c in all_cols if c.startswith(('c2s_', 's2c_', 'flow_', 'total_'))])\n",
    "    gamma_cols = sorted([c for c in all_cols if c.startswith('burst_')])\n",
    "\n",
    "    feature_sets = {\n",
    "        \"Alpha'' (\u03b1'') only\": alpha_cols,\n",
    "        \"Delta (\u03b4) only\": delta_cols,\n",
    "        \"Gamma' (\u03b3') only\": gamma_cols,\n",
    "        \"Alpha'' + Delta\": alpha_cols + delta_cols,\n",
    "        \"Alpha'' + Gamma'\": alpha_cols + gamma_cols,\n",
    "        \"Delta + Gamma'\": delta_cols + gamma_cols,\n",
    "        \"Full (\u03b1'' + \u03b4 + \u03b3')\": alpha_cols + delta_cols + gamma_cols,\n",
    "    }\n",
    "\n",
    "    return df, feature_sets\n",
    "\n",
    "# --- PART 4: Tuned Classification Task ---\n",
    "\n",
    "def run_tuned_classification(df_train, df_test, target_label, feature_set_name, feature_cols, model_name, config):\n",
    "    print(f\" > Training {model_name} on {feature_set_name} ({len(feature_cols)} feats)...\")\n",
    "\n",
    "    X_train = df_train[feature_cols].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    y_train = df_train[target_label]\n",
    "    X_test = df_test[feature_cols].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    y_test = df_test[target_label]\n",
    "\n",
    "    # Encode labels (Union of train/test to handle missing classes in splits)\n",
    "    le = LabelEncoder()\n",
    "    all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "    le.fit(all_labels)\n",
    "    y_train_encoded = le.transform(y_train)\n",
    "    y_test_encoded = le.transform(y_test)\n",
    "    class_names = [str(c) for c in le.classes_]\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', config[\"model\"])\n",
    "    ])\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        pipeline,\n",
    "        param_distributions=config[\"params\"],\n",
    "        n_iter=N_ITER_SEARCH,\n",
    "        scoring='f1_macro',\n",
    "        cv=CV_FOLDS,\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        search.fit(X_train, y_train_encoded)\n",
    "        best_model = search.best_estimator_\n",
    "        best_params = search.best_params_\n",
    "        y_pred = best_model.predict(X_test)\n",
    "\n",
    "        report = classification_report(y_test_encoded, y_pred, target_names=class_names, output_dict=True)\n",
    "        \n",
    "        return {\n",
    "            'model': model_name,\n",
    "            'feature_set': feature_set_name,\n",
    "            'accuracy': report['accuracy'],\n",
    "            'f1_weighted': report['weighted avg']['f1-score'],\n",
    "            'f1_macro': report['macro avg']['f1-score'],\n",
    "            'time': time.time() - start_time,\n",
    "            'best_params': str(best_params).replace(\"classifier__\", \"\")\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"    ERROR in training: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- PART 5: Main Orchestration ---\n",
    "\n",
    "def print_results(task_name, metrics_list):\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"--- FINAL RESULTS: {task_name} ---\")\n",
    "    print(f\"{'='*100}\\n\")\n",
    "    \n",
    "    sorted_res = sorted([m for m in metrics_list if m], key=lambda x: x['f1_macro'], reverse=True)\n",
    "    \n",
    "    print(f\"{'Model':<15} | {'Feature Set':<22} | {'Acc':<6} | {'F1(W)':<6} | {'F1(Mac)':<8} | {'Time':<5}\")\n",
    "    print(\"-\" * 80)\n",
    "    for r in sorted_res:\n",
    "        print(f\"{r['model']:<15} | {r['feature_set']:<22} | {r['accuracy']:.4f} | {r['f1_weighted']:.4f} | {r['f1_macro']:.4f}   | {r['time']:<5.1f}\")\n",
    "    \n",
    "    print(f\"\\n--- \ud83c\udfc6 Top 3 Models for {task_name} ---\")\n",
    "    for i, r in enumerate(sorted_res[:3]):\n",
    "        print(f\"{i+1}. {r['model']} [{r['feature_set']}] - Acc: {r['accuracy']:.4f}, Macro F1: {r['f1_macro']:.4f}\")\n",
    "        print(f\"   Params: {r['best_params']}\")\n",
    "\n",
    "def main():\n",
    "    # 1. Load Data\n",
    "    df, feature_sets = load_data_and_features()\n",
    "    if df is None: return\n",
    "\n",
    "    # 2. Merge with Split Map (Global Split)\n",
    "    if os.path.exists(SPLIT_MAP_FILE):\n",
    "        print(f\"\\n--- Loading Split Map: {SPLIT_MAP_FILE} ---\")\n",
    "        split_map = pd.read_csv(SPLIT_MAP_FILE)\n",
    "        # Normalization (Fix for Case Sensitivity)\n",
    "        split_map.columns = [c.lower().strip() for c in split_map.columns]\n",
    "        \n",
    "        col_map = {'filename': None, 'split': None}\n",
    "        for c in split_map.columns:\n",
    "            if c in ['filename', 'file', 'pcap']: col_map['filename'] = c\n",
    "            if c in ['split', 'set', 'partition', 'split_group']: col_map['split'] = c\n",
    "            \n",
    "        if not col_map['filename'] or not col_map['split']:\n",
    "            print(\"ERROR: Invalid Split Map Columns\")\n",
    "            return\n",
    "            \n",
    "        split_map = split_map.rename(columns={col_map['filename']: 'filename', col_map['split']: 'split'})\n",
    "        split_map['split'] = split_map['split'].str.lower() # Vital Fix\n",
    "        \n",
    "        df_merged = pd.merge(df, split_map[['filename', 'split']], on='filename', how='inner')\n",
    "        print(f\"Data shape after merge: {df_merged.shape}\")\n",
    "    else:\n",
    "        print(\"FATAL ERROR: Split map not found. Aborting to prevent leakage.\")\n",
    "        return\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # STEP 1: BINARY CLASSIFICATION (VPN vs NonVPN)\n",
    "    # ---------------------------------------------------------\n",
    "    print(f\"\\n{'#'*40}\")\n",
    "    print(\" STEP 1: BINARY CLASSIFICATION (VPN vs NonVPN) \")\n",
    "    print(f\"{'#'*40}\")\n",
    "    \n",
    "    df_train = df_merged[df_merged['split'] == 'train']\n",
    "    df_test = df_merged[df_merged['split'] == 'test']\n",
    "    print(f\"Binary Train Samples: {len(df_train)}, Test Samples: {len(df_test)}\")\n",
    "    \n",
    "    binary_metrics = []\n",
    "    for model_name, config in MODEL_CONFIGS.items():\n",
    "        for fs_name, fs_cols in feature_sets.items():\n",
    "            if fs_cols:\n",
    "                m = run_tuned_classification(df_train, df_test, 'binary_type', fs_name, fs_cols, model_name, config)\n",
    "                if m: binary_metrics.append(m)\n",
    "    \n",
    "    print_results(\"Binary Type\", binary_metrics)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # STEP 2: VPN-SPECIFIC CLASSIFICATION (Category & App)\n",
    "    # ---------------------------------------------------------\n",
    "    print(f\"\\n{'#'*40}\")\n",
    "    print(\" STEP 2: VPN-SPECIFIC CLASSIFICATION \")\n",
    "    print(f\"{'#'*40}\")\n",
    "    \n",
    "    # Filter: Keep only VPN rows\n",
    "    df_vpn = df_merged[df_merged['binary_type'] == 'VPN'].copy()\n",
    "    \n",
    "    # Filter: Keep only Target Apps\n",
    "    print(f\"Filtering for specific apps: {TARGET_VPN_APPS}\")\n",
    "    df_vpn_filtered = df_vpn[df_vpn['application'].isin(TARGET_VPN_APPS)]\n",
    "    \n",
    "    df_train_vpn = df_vpn_filtered[df_vpn_filtered['split'] == 'train']\n",
    "    df_test_vpn = df_vpn_filtered[df_vpn_filtered['split'] == 'test']\n",
    "    \n",
    "    print(f\"VPN Filtered Data - Total: {len(df_vpn_filtered)}\")\n",
    "    print(f\"VPN Train Samples: {len(df_train_vpn)}, Test Samples: {len(df_test_vpn)}\")\n",
    "    \n",
    "    if len(df_train_vpn) == 0 or len(df_test_vpn) == 0:\n",
    "        print(\"WARNING: No samples left after filtering for specific VPN apps. Check dataset labels.\")\n",
    "        return\n",
    "\n",
    "    # Run Tasks\n",
    "    for task in ['category', 'application']:\n",
    "        print(f\"\\n--- Target: {task} (Within VPN) ---\")\n",
    "        task_metrics = []\n",
    "        for model_name, config in MODEL_CONFIGS.items():\n",
    "            for fs_name, fs_cols in feature_sets.items():\n",
    "                if fs_cols:\n",
    "                    m = run_tuned_classification(df_train_vpn, df_test_vpn, task, fs_name, fs_cols, model_name, config)\n",
    "                    if m: task_metrics.append(m)\n",
    "        \n",
    "        print_results(f\"VPN {task}\", task_metrics)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fix leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}